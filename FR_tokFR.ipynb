{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f56b62-301a-4a14-996a-dce8ee1f832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download required imports\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchcodec\n",
    "!{sys.executable} -m pip install evaluate\n",
    "!{sys.executable} -m pip install jiwer\n",
    "!{sys.executable} -m pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914a7e39-b54b-4fa2-bd7f-2036ad564390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 5389/5389 [00:00<00:00, 7221.47 examples/s]\n",
      "Generating test split: 100%|██████████| 1348/1348 [00:00<00:00, 6838.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 1348\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Import dataset and split\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"train\")\n",
    "common_voice[\"test\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"test\")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14bd99b6-e3c5-47aa-88c5-d67994120352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 1348\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Keep only needed data (audio and text)\n",
    "common_voice = common_voice.remove_columns([\"audio_filepath\", \"__index_level_0__\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122ec3bc-dc4f-467c-98aa-1d15b12fddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load feature extractor from pre-trained check-point\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0d2943-9973-438c-b78a-f20b5d38aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420b4884-4464-4eb7-b047-da254252d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine feature extractor and tokenizer to create the processor\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f5dd0a-c8e9-4b83-bb98-3c4f690bdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array shape: torch.Size([36784])\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "sample = common_voice[\"train\"][2]\n",
    "audio_decoder = sample['audio']\n",
    "audio_samples = audio_decoder.get_all_samples()\n",
    "\n",
    "audio_array = audio_samples.data.squeeze()\n",
    "sampling_rate = audio_samples.sample_rate\n",
    "\n",
    "print(f\"Audio array shape: {audio_array.shape}\")\n",
    "print(f\"Sampling rate: {sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aececaee-e16d-4b67-968b-d1a97ebfdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    audio_samples = audio.get_all_samples()\n",
    "    audio_array = audio_samples.data.squeeze(0).numpy()\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a78d5f0-d3fb-4670-94f4-38d7533bae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=1): 100%|██████████| 5389/5389 [01:18<00:00, 68.69 examples/s]\n",
      "Map (num_proc=1): 100%|██████████| 1348/1348 [00:20<00:00, 67.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Apply data preparation function to training examples\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bef0130-f4c5-4b7a-9dcc-d267c8ec8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8db9345-9824-4d6d-aa0f-3ccaa6b335f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable automatic language detection and force model to generate french\n",
    "model.generation_config.language = \"french\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a2f442d-13d8-4f76-aa50-939fac1cdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data collector\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f65cf5-5387-45fa-a545-49d857e79291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data collector\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "257b2fe1-9a5a-4d78-a895-7223f739b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.13kB [00:00, 17.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Choose evaluation metrics (error rate)\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2cab28b-5da7-45a5-a9fb-340566f1acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that takes the model's predictions and returns the evaluation metric\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee0f0451-e7d7-46bf-aebe-6f0562a0694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "ckpt_dir = \"/scratch/lemun9@ulaval.ca/whisper_checkpoints\"\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6593a9cd-3305-4c24-9abb-5d654fc27c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training configuration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"/scratch/lemun9@ulaval.ca/whisper_checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    eval_strategy=\"steps\", \n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=2000,\n",
    "    eval_steps=2000, \n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=25,\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    save_only_model=True,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "289a0265-c97d-43d2-9b1f-5a3f0208ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#Forward training agruments, model, dataset, data collector, and compute metrics function to HuggingFace trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4655ac97-5091-485a-8b70-980320cecf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save processor object\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199ae868-f059-49c8-8dfc-a5376d554a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make space for the model training\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7e2fa0b-de88-49b3-aad3-a6a1f25f294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 2:08:54, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.849176</td>\n",
       "      <td>52.090543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>1.051465</td>\n",
       "      <td>52.374199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/ulaval.ca/lemun9/env_hf/lib/python3.12/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.4412349570989609, metrics={'train_runtime': 7736.5838, 'train_samples_per_second': 4.136, 'train_steps_per_second': 0.517, 'total_flos': 9.2304040292352e+18, 'train_loss': 0.4412349570989609, 'epoch': 5.93523844869178})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f48fa1-97b4-4947-9989-b410ba0e1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lemun9/env_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load trained model\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"/scratch/lemun9@ulaval.ca/whisper_checkpoints\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/scratch/lemun9@ulaval.ca/whisper_checkpoints/checkpoint-4000\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad26517c-7099-443e-9124-609bf28c84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_audio_with(model, processor, audio_array):\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(inputs[\"input_features\"], max_length=225)\n",
    "\n",
    "    txt = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b4ae7d-ae35-4618-a3f8-1cc08d3336ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Compare with test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[1;32m      4\u001b[0m common_voice_raw \u001b[38;5;241m=\u001b[39m DatasetDict()\n\u001b[1;32m      5\u001b[0m common_voice_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrishabbahal/quebecois_canadian_french_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.4.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "#Compare with test data\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice_raw = DatasetDict()\n",
    "common_voice_raw[\"train\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", split=\"train\")\n",
    "common_voice_raw[\"test\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", split=\"test\")\n",
    "\n",
    "sample = common_voice_raw[\"test\"][1]\n",
    "audio = sample[\"audio\"][\"array\"]\n",
    "sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "expected_text = sample[\"text\"]\n",
    "print(\"Reference text :\", expected_text)\n",
    "\n",
    "txt_ft = transcribe_from_audio_with(model, processor, audio)\n",
    "\n",
    "print(\"Fine-tuned :\", txt_ft)\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "wer_ft = metric.compute(predictions=[txt_ft], references=[expected_text])\n",
    "\n",
    "print(\"WER fine tuned:\", wer_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf0c62d-5633-4bff-aa0a-b581cbf8650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1348 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n",
      "Processing: 100%|██████████| 1348/1348 [05:59<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "WER fine-tuned: 52.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Make sure models are on GPU\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "def compute_dataset_wer(model, processor, dataset):\n",
    "    preds = []\n",
    "    refs = []\n",
    "    for item in tqdm(dataset, desc=\"Processing\"):\n",
    "        audio = item[\"audio\"][\"array\"]\n",
    "        text = item[\"text\"]\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            ids = model.generate(inputs[\"input_features\"].to(\"cuda\"), max_length=225)\n",
    "        pred = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "        preds.append(pred)\n",
    "        refs.append(text)\n",
    "    return wer(refs, preds)\n",
    "\n",
    "# Use the RAW dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "common_voice_raw = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", split=\"test\")\n",
    "\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "wer_finetuned = compute_dataset_wer(model, processor, common_voice_raw)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"WER fine-tuned: {wer_finetuned*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cb51f-bf14-4a32-a058-2d219ee7bce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_hf)",
   "language": "python",
   "name": "env_hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
