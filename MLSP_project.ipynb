{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f56b62-301a-4a14-996a-dce8ee1f832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: torch in ./env_hf/lib/python3.12/site-packages (2.7.1+computecanada)\n",
      "Requirement already satisfied: filelock in ./env_hf/lib/python3.12/site-packages (from torch) (3.20.0+computecanada)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./env_hf/lib/python3.12/site-packages (from torch) (4.15.0+computecanada)\n",
      "Requirement already satisfied: setuptools in ./env_hf/lib/python3.12/site-packages (from torch) (80.9.0+computecanada)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env_hf/lib/python3.12/site-packages (from torch) (1.14.0+computecanada)\n",
      "Requirement already satisfied: networkx in ./env_hf/lib/python3.12/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./env_hf/lib/python3.12/site-packages (from torch) (3.1.6+computecanada)\n",
      "Requirement already satisfied: fsspec in ./env_hf/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0+computecanada)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env_hf/lib/python3.12/site-packages (from jinja2->torch) (3.0.2+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: torchcodec in ./env_hf/lib/python3.12/site-packages (0.7.0+computecanada)\n",
      "Requirement already satisfied: torch~=2.7.0 in ./env_hf/lib/python3.12/site-packages (from torchcodec) (2.7.1+computecanada)\n",
      "Requirement already satisfied: filelock in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (3.20.0+computecanada)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (4.15.0+computecanada)\n",
      "Requirement already satisfied: setuptools in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (80.9.0+computecanada)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (1.14.0+computecanada)\n",
      "Requirement already satisfied: networkx in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (3.1.6+computecanada)\n",
      "Requirement already satisfied: fsspec in ./env_hf/lib/python3.12/site-packages (from torch~=2.7.0->torchcodec) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from sympy>=1.13.3->torch~=2.7.0->torchcodec) (1.3.0+computecanada)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env_hf/lib/python3.12/site-packages (from jinja2->torch~=2.7.0->torchcodec) (3.0.2+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: evaluate in ./env_hf/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./env_hf/lib/python3.12/site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from evaluate) (1.26.4+computecanada)\n",
      "Requirement already satisfied: dill in ./env_hf/lib/python3.12/site-packages (from evaluate) (0.4.0+computecanada)\n",
      "Requirement already satisfied: pandas in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from evaluate) (2.2.1+computecanada)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./env_hf/lib/python3.12/site-packages (from evaluate) (2.32.5+computecanada)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./env_hf/lib/python3.12/site-packages (from evaluate) (4.67.1+computecanada)\n",
      "Requirement already satisfied: xxhash in ./env_hf/lib/python3.12/site-packages (from evaluate) (3.5.0+computecanada)\n",
      "Requirement already satisfied: multiprocess in ./env_hf/lib/python3.12/site-packages (from evaluate) (0.70.18+computecanada)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./env_hf/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./env_hf/lib/python3.12/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from evaluate) (24.1+computecanada)\n",
      "Requirement already satisfied: filelock in ./env_hf/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.20.0+computecanada)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/arrow/21.0.0/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./env_hf/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.28.1+computecanada)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env_hf/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2+computecanada)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./env_hf/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15+computecanada)\n",
      "Requirement already satisfied: anyio in ./env_hf/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0+computecanada)\n",
      "Requirement already satisfied: certifi in ./env_hf/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./env_hf/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9+computecanada)\n",
      "Requirement already satisfied: idna in ./env_hf/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11+computecanada)\n",
      "Requirement already satisfied: h11>=0.16 in ./env_hf/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0+computecanada)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env_hf/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0+computecanada)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./env_hf/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10+computecanada)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1+computecanada)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0+computecanada)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0+computecanada)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0+computecanada)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0+computecanada)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1+computecanada)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env_hf/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0+computecanada)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_hf/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.4+computecanada)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_hf/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.5.0+computecanada)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./env_hf/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1+computecanada)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0+computecanada)\n",
      "Requirement already satisfied: pytz>=2020.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from pandas->evaluate) (2024.1+computecanada)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from pandas->evaluate) (2024.1+computecanada)\n",
      "Requirement already satisfied: six>=1.5 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: jiwer in ./env_hf/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in ./env_hf/lib/python3.12/site-packages (from jiwer) (8.3.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in ./env_hf/lib/python3.12/site-packages (from jiwer) (3.13.0+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: transformers[torch] in ./env_hf/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (3.20.0+computecanada)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from transformers[torch]) (1.26.4+computecanada)\n",
      "Requirement already satisfied: packaging>=20.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from transformers[torch]) (24.1+computecanada)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (6.0.2+computecanada)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (2024.11.6+computecanada)\n",
      "Requirement already satisfied: requests in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (2.32.5+computecanada)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (0.22.1+computecanada)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (0.5.3+computecanada)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (4.67.1+computecanada)\n",
      "Requirement already satisfied: torch>=2.2 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (2.7.1+computecanada)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./env_hf/lib/python3.12/site-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env_hf/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env_hf/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0+computecanada)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./env_hf/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10+computecanada)\n",
      "Requirement already satisfied: psutil in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2024a/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: setuptools in ./env_hf/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (80.9.0+computecanada)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env_hf/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.14.0+computecanada)\n",
      "Requirement already satisfied: networkx in ./env_hf/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./env_hf/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.1.6+computecanada)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0+computecanada)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env_hf/lib/python3.12/site-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.2+computecanada)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_hf/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.4+computecanada)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env_hf/lib/python3.12/site-packages (from requests->transformers[torch]) (3.11+computecanada)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_hf/lib/python3.12/site-packages (from requests->transformers[torch]) (2.5.0+computecanada)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env_hf/lib/python3.12/site-packages (from requests->transformers[torch]) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "#Download required imports\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchcodec\n",
    "!{sys.executable} -m pip install evaluate\n",
    "!{sys.executable} -m pip install jiwer\n",
    "!{sys.executable} -m pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914a7e39-b54b-4fa2-bd7f-2036ad564390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lemun9/env_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 5389/5389 [00:01<00:00, 3294.87 examples/s]\n",
      "Generating test split: 100%|██████████| 1348/1348 [00:00<00:00, 7728.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 1348\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Import dataset and split\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"train\")\n",
    "common_voice[\"test\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"test\")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd99b6-e3c5-47aa-88c5-d67994120352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only needed data (audio and text)\n",
    "common_voice = common_voice.remove_columns([\"audio_filepath\", \"__index_level_0__\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122ec3bc-dc4f-467c-98aa-1d15b12fddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load feature extractor from pre-trained check-point\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0d2943-9973-438c-b78a-f20b5d38aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420b4884-4464-4eb7-b047-da254252d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine feature extractor and tokenizer to create the processor\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f5dd0a-c8e9-4b83-bb98-3c4f690bdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array shape: torch.Size([36784])\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "sample = common_voice[\"train\"][2]\n",
    "audio_decoder = sample['audio']\n",
    "audio_samples = audio_decoder.get_all_samples()\n",
    "\n",
    "audio_array = audio_samples.data.squeeze()\n",
    "sampling_rate = audio_samples.sample_rate\n",
    "\n",
    "print(f\"Audio array shape: {audio_array.shape}\")\n",
    "print(f\"Sampling rate: {sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aececaee-e16d-4b67-968b-d1a97ebfdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    audio_samples = audio_decoder.get_all_samples()\n",
    "    audio_array = audio_samples.data.squeeze(0).numpy()\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a78d5f0-d3fb-4670-94f4-38d7533bae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=1): 100%|██████████| 5389/5389 [01:22<00:00, 65.70 examples/s]\n",
      "Map (num_proc=1): 100%|██████████| 1348/1348 [00:20<00:00, 65.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Apply data preparation function to training examples\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bef0130-f4c5-4b7a-9dcc-d267c8ec8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8db9345-9824-4d6d-aa0f-3ccaa6b335f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable automatic language detection and force model to generate french\n",
    "model.generation_config.language = \"french\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2f442d-13d8-4f76-aa50-939fac1cdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data collector\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f65cf5-5387-45fa-a545-49d857e79291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data collector\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257b2fe1-9a5a-4d78-a895-7223f739b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.13kB [00:00, 19.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Choose evaluation metrics (error rate)\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2cab28b-5da7-45a5-a9fb-340566f1acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that takes the model's predictions and returns the evaluation metric\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6593a9cd-3305-4c24-9abb-5d654fc27c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training configuration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"/scratch/lemun9@ulaval.ca/whisper_checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    eval_strategy=\"steps\", \n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=1000,\n",
    "    eval_steps=1000, \n",
    "    gradient_checkpointing=False,\n",
    "    fp16=False,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=25,\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    save_only_model=True,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289a0265-c97d-43d2-9b1f-5a3f0208ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#Forward training agruments, model, dataset, data collector, and compute metrics function to HuggingFace trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4655ac97-5091-485a-8b70-980320cecf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save processor object\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "199ae868-f059-49c8-8dfc-a5376d554a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make space for the model training\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7e2fa0b-de88-49b3-aad3-a6a1f25f294c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/accelerate/accelerator.py:2848\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2848\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/utils/checkpoint.py:320\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    322\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_hf/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f48fa1-97b4-4947-9989-b410ba0e1013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_hf)",
   "language": "python",
   "name": "env_hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
