{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f56b62-301a-4a14-996a-dce8ee1f832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download required imports\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchcodec\n",
    "!{sys.executable} -m pip install evaluate\n",
    "!{sys.executable} -m pip install jiwer\n",
    "!{sys.executable} -m pip install transformers[torch]\n",
    "!{sys.executable} -m pip install soundfile\n",
    "!{sys.executable} -m pip install torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97673492-c186-4d56-9246-27c006eeadd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: 4.4.1\n",
      "huggingface_hub: 0.36.0\n"
     ]
    }
   ],
   "source": [
    "import datasets, huggingface_hub\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "914a7e39-b54b-4fa2-bd7f-2036ad564390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text', 'audio_filepath', '__index_level_0__'],\n",
      "        num_rows: 1348\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Import dataset and split\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"train\")\n",
    "common_voice[\"test\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", \"default\", split=\"test\")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14bd99b6-e3c5-47aa-88c5-d67994120352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 1348\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Keep only needed data (audio and text)\n",
    "common_voice = common_voice.remove_columns([\"audio_filepath\", \"__index_level_0__\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122ec3bc-dc4f-467c-98aa-1d15b12fddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load feature extractor from pre-trained check-point\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b0d2943-9973-438c-b78a-f20b5d38aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420b4884-4464-4eb7-b047-da254252d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine feature extractor and tokenizer to create the processor\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"French\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f5dd0a-c8e9-4b83-bb98-3c4f690bdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "sample = common_voice[\"train\"][2]\n",
    "audio_decoder = sample['audio']\n",
    "#audio_samples = audio_decoder.get_all_samples()\n",
    "\n",
    "sampling_rate = audio_decoder[\"sampling_rate\"] \n",
    "\n",
    "print(f\"Sampling rate: {sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aececaee-e16d-4b67-968b-d1a97ebfdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset\n",
    "def prepare_dataset(batch):\n",
    "\n",
    "    audio_array = batch[\"audio\"][\"array\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63729344-ec6b-4b4d-8d61-7d5d2e7f1048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['audio', 'text'])\n",
      "Success! Result keys: dict_keys(['audio', 'text', 'input_features', 'labels'])\n",
      "Input features shape: (80, 3000)\n",
      "Labels: [50258, 50265, 50359, 50363, 50257]\n",
      "Current columns: ['audio', 'text']\n"
     ]
    }
   ],
   "source": [
    "test_example = common_voice[\"train\"][0]\n",
    "print(\"Original keys:\", test_example.keys())\n",
    "\n",
    "try:\n",
    "    result = prepare_dataset(test_example)\n",
    "    print(\"Success! Result keys:\", result.keys())\n",
    "    print(\"Input features shape:\", result[\"input_features\"].shape)\n",
    "    print(\"Labels:\", result[\"labels\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"Current columns:\", common_voice[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a78d5f0-d3fb-4670-94f4-38d7533bae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply data preparation function to training examples\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bef0130-f4c5-4b7a-9dcc-d267c8ec8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8db9345-9824-4d6d-aa0f-3ccaa6b335f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable automatic language detection and force model to generate french\n",
    "model.generation_config.language = \"french\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff1096e4-6e08-4eb7-bb6b-3b707ad097cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 241,734,912\n",
      "Trainable parameters: 28,352,256\n",
      "Frozen parameters: 213,382,656\n",
      "Percentage trainable: 11.73%\n"
     ]
    }
   ],
   "source": [
    "# freeze 100% of the encoder\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# freeze 80% of the decoder\n",
    "decoder_layers = model.model.decoder.layers\n",
    "total_layers = len(decoder_layers)\n",
    "layers_to_freeze = int(total_layers * 0.8)\n",
    "\n",
    "for i in range(layers_to_freeze):\n",
    "    for param in decoder_layers[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# freeze embedding layer\n",
    "for param in model.model.decoder.embed_tokens.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.model.decoder.embed_positions.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2f442d-13d8-4f76-aa50-939fac1cdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data collector\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # create attention mask for encoder inputs\n",
    "        attention_mask = torch.ones(batch[\"input_features\"].shape[:-1], dtype=torch.long)\n",
    "        is_padding = (batch[\"input_features\"] == 0).all(dim=-1)\n",
    "        attention_mask[is_padding] = 0\n",
    "        batch[\"attention_mask\"] = attention_mask\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f65cf5-5387-45fa-a545-49d857e79291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data collector\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257b2fe1-9a5a-4d78-a895-7223f739b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose evaluation metrics (error rate)\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cab28b-5da7-45a5-a9fb-340566f1acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that takes the model's predictions and returns the evaluation metric\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee0f0451-e7d7-46bf-aebe-6f0562a0694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "ckpt_dir = \"/scratch/lemun9@ulaval.ca/whisper_checkpoints\"\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6593a9cd-3305-4c24-9abb-5d654fc27c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training configuration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"/scratch/lemun9@ulaval.ca/whisper_checkpoints\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    eval_strategy=\"steps\", \n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=1000,\n",
    "    eval_steps=500, \n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=25,\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    save_only_model=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "model.gradient_checkpointing_disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289a0265-c97d-43d2-9b1f-5a3f0208ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#Forward training agruments, model, dataset, data collector, and compute metrics function to HuggingFace trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4655ac97-5091-485a-8b70-980320cecf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save processor object\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199ae868-f059-49c8-8dfc-a5376d554a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make space for the model training\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e2fa0b-de88-49b3-aad3-a6a1f25f294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 2:42:38, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.235800</td>\n",
       "      <td>1.224333</td>\n",
       "      <td>60.594542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.118990</td>\n",
       "      <td>58.540875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>1.081819</td>\n",
       "      <td>57.627503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>1.060266</td>\n",
       "      <td>57.701254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>1.053600</td>\n",
       "      <td>55.483066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>1.052394</td>\n",
       "      <td>58.098372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>1.047832</td>\n",
       "      <td>56.623362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>1.048281</td>\n",
       "      <td>56.838940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lemun9/env_hf/lib/python3.12/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.9608395533561707, metrics={'train_runtime': 9767.5105, 'train_samples_per_second': 3.276, 'train_steps_per_second': 0.41, 'total_flos': 9.2304040292352e+18, 'train_loss': 0.9608395533561707, 'epoch': 5.935064935064935})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87612fb8-5291-4f35-83ae-b638dda469b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load trained model\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/scratch/lemun9@ulaval.ca/whisper_checkpoints/checkpoint-4000\")\n",
    "model_proc = WhisperProcessor.from_pretrained(\"/scratch/lemun9@ulaval.ca/whisper_checkpoints\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acca16af-4081-431a-b09f-b1ce7a13666b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load original model\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "base_proc = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2665e465-f098-4054-b062-7ba81cb7e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "def transcribe_from_path_with(model, processor, path):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        audio = resampler(audio)\n",
    "\n",
    "    audio = audio.squeeze().numpy()\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(inputs[\"input_features\"], max_length=225)\n",
    "\n",
    "    text = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccccba2f-9505-42a3-9ed1-afb8cc78e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine tuned model transcription:\n",
      "T'es tellement beau les étoiles. Garde, c'est la grande ours. Puis, ça c'est quoi? Ça c'est... Attends, c'est la tête ça! Je te l'avais dit qu'on n'avait pas pris le bon chemin. Ça à gauche, il faut continuer de se trouver pour aller au parc. Je pense que c'est tout droit, parce qu'à gauche, il a l'air d'avoir un vortex spatio-temporel qui mène vers une autre planète. Non, c'est sûr que c'est un vortex qui mène au parc. S'il y a autre l'effet,\n",
      "\n",
      "original model transcription:\n",
      " C'est tellement beau, les étoiles. Hum. Garde, c'est la grande ours. Oh! Et ça, c'est quoi? Ça, c'est... Attends, c'est la terre, ça! Je t'avais dit qu'on n'avait pas pris le bon chemin. Ça, à gauche, il faut continuer de tourner par aupar. Ben, je pense que c'est tout droit parce qu'à gauche, c'est l'heure d'avoir un vortex spatio-temporel qui mène vers une autre planète. Non, c'est sûr que c'est un vortex qui mène à haupar. Toute l'effet, vous avez pas le sens d'orientation.\n"
     ]
    }
   ],
   "source": [
    "path = \"audioQc.mp3\"\n",
    "\n",
    "txt_finetuned = transcribe_from_path_with(model, model_proc, path)\n",
    "txt_base = transcribe_from_path_with(base_model, base_proc, path)\n",
    "\n",
    "print(\"fine tuned model transcription:\")\n",
    "print(txt_finetuned)\n",
    "print()\n",
    "print(\"original model transcription:\")\n",
    "print(txt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bc8a9f0-80b8-4113-b7db-8352a761112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_audio_with(model, processor, audio_array):\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(inputs[\"input_features\"], max_length=225)\n",
    "\n",
    "    txt = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6050348e-ef09-4ac9-a932-c3330f6324ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference text : l'île qu'on voit pas du côté sud c'est\n",
      "magnifique les champs fleurs les maisons mais\n",
      "Fine-tuned : l'île qu'on voit pas du côté sud. C'est magnifique les champs de fleurs, les maisons, mais\n",
      "Original :  qu'on voit pas du côté sud. C'est magnifique les champs de fleurs, les maisons, les canons.\n",
      "WER fine tuned: 0.42857142857142855\n",
      "WER original: 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "#Compare with test data\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice_raw = DatasetDict()\n",
    "common_voice_raw[\"train\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", split=\"train\")\n",
    "common_voice_raw[\"test\"] = load_dataset(\"rishabbahal/quebecois_canadian_french_dataset\", split=\"test\")\n",
    "\n",
    "sample = common_voice_raw[\"train\"][1]\n",
    "audio = sample[\"audio\"][\"array\"]\n",
    "sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "expected_text = sample[\"text\"]\n",
    "print(\"Reference text :\", expected_text)\n",
    "\n",
    "txt_ft = transcribe_from_audio_with(model, processor, audio)\n",
    "txt_orig = transcribe_from_audio_with(base_model, base_proc, audio)\n",
    "\n",
    "print(\"Fine-tuned :\", txt_ft)\n",
    "print(\"Original :\", txt_orig)\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "wer_ft = metric.compute(predictions=[txt_ft], references=[expected_text])\n",
    "wer_orig = metric.compute(predictions=[txt_orig], references=[expected_text])\n",
    "\n",
    "print(\"WER fine tuned:\", wer_ft)\n",
    "print(\"WER original:\", wer_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cad51b-fea0-4dec-85d3-d083ff15b1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_hf)",
   "language": "python",
   "name": "env_hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
